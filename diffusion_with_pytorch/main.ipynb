{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_swiss_roll # this give a 3d data sample that is in the sape of swiss roll. \n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(batch_size,device = 'cpu'):\n",
    "    data, _ = make_swiss_roll(batch_size)  # We generate a data that looks like a swiss rolle with n_samples number of data points. \n",
    "    data = data[:,[2,0]]/10\n",
    "    data = data * np.array([1,-1])\n",
    "    return(torch.from_numpy(data).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, N = 40, data_dim = 2, hidden_dim = 64):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.network_head = nn.Sequential(nn.Linear(data_dim,hidden_dim),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(hidden_dim,hidden_dim),\n",
    "                                          nn.ReLU(),)\n",
    "        self.network_tail = nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim,hidden_dim),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(hidden_dim,data_dim * 2),) for t in range(N)])\n",
    "\n",
    "    def forward(self,x,t):\n",
    "\n",
    "        h = self.network_head(x) # [batch_size, hidden_dim]\n",
    "        tmp = self.network_tail[t](h) # [batch_size, data_dim *2]\n",
    "        mu,h = torch.chunk(tmp,2,dim=1)\n",
    "        var = torch.exp(h)\n",
    "        std = torch.sqrt(var)\n",
    "        return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP()\n",
    "# torch.save(model,'model_paper1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (network_head): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (network_tail): ModuleList(\n",
       "    (0-39): 40 x Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward diffusion Kernel \n",
    "\n",
    "$$\\mathcal{N}\\left(x^{(t)},\\underbrace{x^{(t-1)}\\sqrt{1-\\beta_t}}_{\\text{mu}},\\underbrace{I \\beta_t}_{\\text{std}} \\right)$$\n",
    "\n",
    "\n",
    "We are uisng the reparamatriztion tecnic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModule():\n",
    "\n",
    "    def __init__(self,T, model: nn.Module,device,dim = 2):\n",
    "        self.betas = (torch.sigmoid(torch.linspace(-18,10,T)) * (3e-1 - 1e-5) + 1e-5).to(device)\n",
    "        self.alphas = (1 - self.betas).to(device)\n",
    "        self.alphas_bar = torch.cumprod(self.alphas,0).to(device)\n",
    "        self.T = T\n",
    "        self.model = model\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "\n",
    "    def forward_porcess(self,x0, t):\n",
    "        \"\"\"\n",
    "        :param data: data\n",
    "        :param t: Number of diffusion steps\n",
    "        \"\"\"\n",
    "\n",
    "        assert t > 0, 'ArithmeticError: t must be gratter thin zero'\n",
    "\n",
    "        t = t -1 # Becouse we are starting indexing at 0 \n",
    "\n",
    "        mu = torch.sqrt(self.alphas_bar[t]) * x0\n",
    "        std =  torch.sqrt(1-self.alphas_bar[t])\n",
    "        epsilon =  torch.randn_like(x0)\n",
    "        xt = mu + epsilon * std \n",
    " \n",
    "        sigma_q = torch.sqrt(((1-self.alphas_bar[t-1])/(1-self.alphas_bar[t]))*self.betas[t]) \n",
    "        m1 = torch.sqrt(self.alphas_bar[t-1]) * self.betas[t] / (1- self.alphas_bar[t])\n",
    "        m2 =  torch.sqrt(self.alphas[t])* (1- self.alphas_bar[t-1]) / (1- self.alphas_bar[t])\n",
    "        mu_q = m1*x0 + m2 *xt\n",
    "\n",
    "\n",
    "        return mu_q, sigma_q, xt\n",
    "\n",
    "    def reverse_process(self, xt, t):\n",
    "\n",
    "        assert t > 0, 't should be greatter then zero'\n",
    "        assert t <= self.T, 't should be less then or equl to self.T'\n",
    "\n",
    "        t = t - 1 # becouse we are starting indexing at 0 \n",
    "        mu, std = self.model(xt, t)\n",
    "        epsilon =  torch.randn_like(xt)\n",
    "        return mu,std, mu + torch.randn_like(xt) * std \n",
    "\n",
    "    def sample(self,batch_size,device):\n",
    "\n",
    "        noise = (torch.randn((batch_size,self.dim))).to(device)\n",
    "        x = noise \n",
    "\n",
    "        samples = [x]\n",
    "        for t in range(self.T, 0 , -1):\n",
    "            _,_,x = self.reverse_process(x,t)\n",
    "            samples.append(x)\n",
    "\n",
    "        return samples[::-1]\n",
    "\n",
    "    def get_loss(self,x0):\n",
    "        \"\"\" \n",
    "        :param x0: batch [batch_size, self.dim]\n",
    "        \"\"\"\n",
    "\n",
    "        t = torch.randint(2,40+1,(1,)).to(self.device)\n",
    "        mu_q, sigma_q, xt = self.forward_porcess(x0,t)\n",
    "        mu_p, sigma_p, xt_minus1 = self.reverse_process(xt.float(),t)\n",
    "\n",
    "        KL = torch.log(sigma_p) - torch.log(sigma_q) + (sigma_q**2 + (mu_q - mu_p)**2)/(2* sigma_p **2)\n",
    "        K =  - KL.mean() # should be mazimise \n",
    "        loss = - K # should be minimised \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(range(40,0,-1))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion_model = DiffusionModule(40,mlp_model,'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(samples[2][:,0].data.numpy(), samples[2][:,1].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model,file_name,device):\n",
    "    fontsize = 14\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    N = 5_000\n",
    "    x0 = sample_batch(N).to(device)\n",
    "    samples = model.sample(N,device = device)\n",
    "\n",
    "\n",
    "    data = [x0.to('cpu'), model.forward_porcess(x0,20)[-1].to('cpu'),model.forward_porcess(x0,40)[-1].to('cpu')]\n",
    "    for i in range(3):\n",
    "        plt.subplot(2,3,1+i)\n",
    "        plt.scatter(data[i][:,0].data.numpy(),data[i][:,1].data.numpy(),alpha=0.1,s=2)\n",
    "        plt.xlim([-2,2])\n",
    "        plt.ylim([-2,2])\n",
    "        plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "        if i == 0: plt.ylabel(r'$q(\\mathbf{x}^{(0...T)})$',fontsize = fontsize)\n",
    "        if i == 0: plt.title(r'$t=0$',fontsize = fontsize)\n",
    "        if i == 1: plt.title(r'$t=\\frac{T}{2}$',fontsize = fontsize)\n",
    "        if i == 2: plt.title(r'$t=T$',fontsize = fontsize)\n",
    "    time_step = [1,20,40]\n",
    "    for i in range(3):\n",
    "        plt.subplot(2,3,4+i)\n",
    "        plt.scatter(samples[time_step[i]][:,0].data.to('cpu').numpy(),samples[time_step[i]][:,1].data.to('cpu').numpy(),alpha=0.1,c= 'r',s=2)\n",
    "        plt.xlim([-2,2])\n",
    "        plt.ylim([-2,2])\n",
    "        plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "        if i == 0: plt.ylabel(r'$P(\\mathbf{x}^{(0...T)})$',fontsize = fontsize)\n",
    "    plt.savefig(file_name,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(Diffusion_model, optimizer, batch_size, nb_epoch,device):\n",
    "\n",
    "    traning_loss = []\n",
    "    traning_loss_helper = []\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        x0 = sample_batch(batch_size,device)\n",
    "        loss = Diffusion_model.get_loss(x0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        traning_loss_helper.append(loss.item())\n",
    "\n",
    "        if epoch% 2000 == 0: \n",
    "            traning_loss.append(np.average(traning_loss_helper))\n",
    "            traning_loss_helper = []\n",
    "            plt.plot(traning_loss)\n",
    "            plt.savefig(f'fig/training_loss_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "\n",
    "            plot(Diffusion_model,f'fig/traning_epoch_{epoch}.png',device)\n",
    "\n",
    "    return traning_loss\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "mlp_model = MLP(hidden_dim=128).to(device)\n",
    "model = DiffusionModule(40,mlp_model,device)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(),lr = 1e-4)\n",
    "_ = train(model, optimizer, 64_000, 300_000,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.cuda.device object at 0x7f3d3e694990>]\n"
     ]
    }
   ],
   "source": [
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model,'model_paper1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-with-pytorch-r8cGOXQq-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
